{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('features.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_input = data[:,0:25]\n",
    "all_output = .5*data[:,25:27]+.5\n",
    "train_input = data[:1000000,0:25]\n",
    "train_output = .5*data[:1000000,25:27]+.5\n",
    "test_input = data[1000000:,0:25]\n",
    "test_output = .5*data[1000000:,25:27]+.5\n",
    "#Don't do this!\n",
    "train_input = all_input\n",
    "train_output = all_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input = data[356048:,0:25]\n",
    "train_output = .5*data[356048:,25:27]+.5\n",
    "test_input = data[:356048,0:25]\n",
    "test_output = .5*data[:356048,25:27]+.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize frames so we can do mini-batches. Don't do that for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:]\n",
    "    shuffled_labels = labels[permutation,:]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_input, train_output = randomize(train_input, train_output)\n",
    "test_input, test_output = randomize(test_input, test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:]\n",
    "    shuffled_labels = labels[permutation,:]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "all_input, all_output = randomize(all_input, all_output)\n",
    "train_input = all_input[:1000000,:]\n",
    "train_output = all_output[:1000000,:]\n",
    "test_input = all_input[1000000:,:]\n",
    "test_output = all_output[1000000:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup MLP definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#batch_size = 1356048\n",
    "batch_size = 13560\n",
    "hidden_size = 16\n",
    "feature_size = 25\n",
    "nb_outputs = 2\n",
    "\n",
    "def my_cross_entropy(x,z):\n",
    "    entropy = x - x * z + tf.log(1 + tf.exp(-x))\n",
    "    #entropy = entropy * (z!=0.5)\n",
    "    entropy = entropy * (2*tf.abs(z-0.5))\n",
    "    entropy = entropy * [5, 1]\n",
    "    return entropy\n",
    "\n",
    "def my_mse(x,z):\n",
    "    mse = (x - z)**2\n",
    "    mse = mse * (2*np.abs(z-0.5))\n",
    "    rms = 5*mse[:,0] + mse[:,1]\n",
    "    return 2*np.sqrt(np.mean(rms))\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(None, feature_size))\n",
    "    tf_train_output = tf.placeholder(tf.float32, shape=(None, nb_outputs))\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([feature_size, hidden_size]))\n",
    "    biases = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "    logits1 = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    hidden = tf.nn.tanh(logits1)\n",
    "\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_size, nb_outputs]))\n",
    "    biases2 = tf.Variable(tf.zeros([nb_outputs]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(hidden, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        my_cross_entropy(logits, tf_train_output))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.sigmoid(logits)\n",
    "    tf_test_dataset = tf.to_float(tf.constant(test_input))\n",
    "    test_prediction = tf.nn.sigmoid(tf.matmul(tf.nn.tanh(tf.matmul(tf_test_dataset, weights) + biases), weights2) + biases2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('step 0:', array([ 0.33901381,  0.6559989 ]), 2.7593063301604341)\n",
      "('step 10000:', array([ 0.01055051,  0.03929876]), 0.5309413611569257)\n",
      "('step 20000:', array([ 0.00821284,  0.03636818]), 0.48530305543486219)\n",
      "('step 30000:', array([ 0.00771875,  0.03502162]), 0.47348787198022779)\n",
      "('step 40000:', array([ 0.00746581,  0.03458727]), 0.46810315897765681)\n",
      "('step 50000:', array([ 0.00735888,  0.03429008]), 0.46477801969079563)\n",
      "('step 60000:', array([ 0.00720402,  0.03418979]), 0.46339541788187744)\n",
      "('step 70000:', array([ 0.00712143,  0.03397151]), 0.46108325315043203)\n",
      "('step 80000:', array([ 0.00685595,  0.03337419]), 0.45583643011508423)\n",
      "('step 90000:', array([ 0.00690389,  0.03287715]), 0.45430868711897104)\n",
      "('step 100000:', array([ 0.00677705,  0.03293762]), 0.45172379367593218)\n",
      "('step 110000:', array([ 0.00681171,  0.03297007]), 0.4526366497010374)\n",
      "('step 120000:', array([ 0.00664873,  0.03267141]), 0.44946324262374254)\n",
      "('step 130000:', array([ 0.00664726,  0.03244723]), 0.44895423040800214)\n",
      "('step 140000:', array([ 0.00665832,  0.03252024]), 0.44907964585795573)\n",
      "('step 150000:', array([ 0.00675197,  0.03244723]), 0.4499590429521535)\n",
      "('step 160000:', array([ 0.0067033 ,  0.03253498]), 0.44932190786477422)\n",
      "('step 170000:', array([ 0.00661997,  0.03217954]), 0.44774083950107507)\n",
      "('step 180000:', array([ 0.00685816,  0.03222157]), 0.45023221871030228)\n",
      "('step 190000:', array([ 0.00670846,  0.03219724]), 0.44830588211385652)\n",
      "('step 200000:', array([ 0.00665021,  0.03206155]), 0.4470826744432071)\n",
      "('step 210000:', array([ 0.0067092 ,  0.03202173]), 0.44750059725552305)\n",
      "('step 220000:', array([ 0.00680802,  0.03171864]), 0.44862337734991531)\n",
      "('step 230000:', array([ 0.00662513,  0.0314598 ]), 0.44415662964488523)\n",
      "('step 240000:', array([ 0.00662882,  0.03145906]), 0.44482504227165665)\n",
      "('step 250000:', array([ 0.0067033 ,  0.03138974]), 0.44491810416241667)\n",
      "('step 260000:', array([ 0.00657425,  0.03133296]), 0.44287602076120375)\n",
      "('step 270000:', array([ 0.00663472,  0.03144726]), 0.44381751039906886)\n",
      "('step 280000:', array([ 0.00666864,  0.03137942]), 0.44426137174733932)\n",
      "('step 290000:', array([ 0.00656466,  0.03126069]), 0.44313346641580065)\n",
      "('step 300000:', array([ 0.00658752,  0.03132706]), 0.442736810502527)\n",
      "('step 310000:', array([ 0.00660154,  0.031375  ]), 0.44428781396763128)\n",
      "('step 320000:', array([ 0.00658679,  0.03155346]), 0.44472980999020023)\n",
      "('step 330000:', array([ 0.00665832,  0.03115598]), 0.44412776976220791)\n",
      "('step 340000:', array([ 0.00657941,  0.03126659]), 0.44259082114307935)\n",
      "('step 350000:', array([ 0.00659637,  0.03135214]), 0.44404062945638328)\n",
      "('step 360000:', array([ 0.00665021,  0.03127249]), 0.44359300048228273)\n",
      "('step 370000:', array([ 0.00654254,  0.03150626]), 0.44321862564306225)\n",
      "('step 380000:', array([ 0.00673133,  0.03137131]), 0.44497889993967749)\n",
      "('step 390000:', array([ 0.00671141,  0.03168988]), 0.44641463406287013)\n",
      "('step 400000:', array([ 0.00662366,  0.03122456]), 0.44335193113917504)\n",
      "('step 410000:', array([ 0.00665537,  0.03123488]), 0.44346112889411854)\n",
      "('step 420000:', array([ 0.00653295,  0.03150552]), 0.44336475262603875)\n",
      "('step 430000:', array([ 0.00660375,  0.03154829]), 0.44463851999427884)\n",
      "('step 440000:', array([ 0.00663841,  0.03134181]), 0.44415632199314276)\n",
      "('step 450000:', array([ 0.00651083,  0.03120981]), 0.44216975048841223)\n",
      "('step 460000:', array([ 0.00660006,  0.03133591]), 0.44402326537189835)\n",
      "('step 470000:', array([ 0.00667675,  0.03126954]), 0.44470945719606753)\n",
      "('step 480000:', array([ 0.00654107,  0.03136393]), 0.44283000705098541)\n",
      "('step 490000:', array([ 0.00661923,  0.03127765]), 0.44362814056200578)\n",
      "('step 500000:', array([ 0.00659932,  0.03144358]), 0.44374250692296985)\n",
      "('step 510000:', array([ 0.00662882,  0.0312806 ]), 0.44513853347863741)\n",
      "('step 520000:', array([ 0.00658015,  0.03129461]), 0.44232775443351841)\n",
      "('step 530000:', array([ 0.00648502,  0.03124594]), 0.44224296221672499)\n",
      "('step 540000:', array([ 0.00655065,  0.03140966]), 0.44184324649396356)\n",
      "('step 550000:', array([ 0.00655065,  0.03211612]), 0.44457931732075312)\n",
      "('step 560000:', array([ 0.00661112,  0.03153944]), 0.44342608571940889)\n",
      "('step 570000:', array([ 0.00658384,  0.03123562]), 0.44256512087290789)\n",
      "('step 580000:', array([ 0.00661702,  0.03137057]), 0.44315423024679351)\n",
      "('step 590000:', array([ 0.00663988,  0.03143989]), 0.44475733060750322)\n",
      "('step 600000:', array([ 0.00668487,  0.03143842]), 0.4444802757438493)\n",
      "('step 610000:', array([ 0.00642308,  0.03120833]), 0.44029896610621777)\n",
      "('step 620000:', array([ 0.00650346,  0.03154461]), 0.44213388664757919)\n",
      "('step 630000:', array([ 0.00660154,  0.03153281]), 0.44450966534263175)\n",
      "('step 640000:', array([ 0.00656319,  0.03147086]), 0.44296999162709449)\n",
      "('step 650000:', array([ 0.00648133,  0.03145759]), 0.44127191936775806)\n",
      "('step 660000:', array([ 0.0064747 ,  0.03128798]), 0.44085504275008486)\n",
      "('step 670000:', array([ 0.00663398,  0.03133812]), 0.44392233125485575)\n",
      "('step 680000:', array([ 0.00652263,  0.03135214]), 0.44188704196443734)\n",
      "('step 690000:', array([ 0.00638252,  0.03154239]), 0.44117150740923511)\n",
      "('step 700000:', array([ 0.00660301,  0.03135951]), 0.44338265945649263)\n",
      "('step 710000:', array([ 0.00666791,  0.03143547]), 0.44524473144579313)\n",
      "('step 720000:', array([ 0.00660596,  0.03141408]), 0.44285235698800995)\n",
      "('step 730000:', array([ 0.00647396,  0.03122825]), 0.44121520186961388)\n",
      "('step 740000:', array([ 0.00654623,  0.03128577]), 0.44186992026777239)\n",
      "('step 750000:', array([ 0.00654402,  0.03125996]), 0.44251656194359218)\n",
      "('step 760000:', array([ 0.00654844,  0.03145685]), 0.44217366080788689)\n",
      "('step 770000:', array([ 0.00638252,  0.03113164]), 0.44019034232770049)\n",
      "('step 780000:', array([ 0.00659342,  0.03126217]), 0.4426543779810439)\n",
      "('step 790000:', array([ 0.00679548,  0.03132706]), 0.44642273099166296)\n",
      "('step 800000:', array([ 0.00644004,  0.03138016]), 0.44093567326064426)\n",
      "('step 810000:', array([ 0.00656761,  0.03132706]), 0.44246200195454927)\n",
      "('step 820000:', array([ 0.00652116,  0.03124668]), 0.44185625944576007)\n",
      "('step 830000:', array([ 0.00657278,  0.03133739]), 0.44244426108917911)\n",
      "('step 840000:', array([ 0.00650493,  0.0312253 ]), 0.44115679989918649)\n",
      "('step 850000:', array([ 0.00643709,  0.03120465]), 0.44073409695307214)\n",
      "('step 860000:', array([ 0.00647691,  0.03129756]), 0.4410285601155543)\n",
      "('step 870000:', array([ 0.00646069,  0.03182188]), 0.44277881220644627)\n",
      "('step 880000:', array([ 0.00676156,  0.03139196]), 0.44572979166180315)\n",
      "('step 890000:', array([ 0.00651157,  0.03124521]), 0.44153115746572569)\n",
      "('step 900000:', array([ 0.00650862,  0.03136762]), 0.4415850397749464)\n",
      "('step 910000:', array([ 0.00650198,  0.03123341]), 0.44132033721969516)\n",
      "('step 920000:', array([ 0.00655434,  0.03148045]), 0.44272146052240469)\n",
      "('step 930000:', array([ 0.00640833,  0.03126438]), 0.44051992302216153)\n",
      "('step 940000:', array([ 0.00651526,  0.03124447]), 0.44131023801199498)\n",
      "('step 950000:', array([ 0.00654697,  0.03127323]), 0.44193729307573754)\n",
      "('step 960000:', array([ 0.00644299,  0.03138163]), 0.44141450741708771)\n",
      "('step 970000:', array([ 0.00652411,  0.031375  ]), 0.44156590095755355)\n",
      "('step 980000:', array([ 0.00651304,  0.03161319]), 0.44265735473398582)\n",
      "('step 990000:', array([ 0.00678295,  0.03133591]), 0.44541946511553598)\n",
      "('step 1000000:', array([ 0.0065536 ,  0.03125332]), 0.44215809018015501)\n",
      "('step 1010000:', array([ 0.00644446,  0.03129978]), 0.44061485639552134)\n",
      "('step 1020000:', array([ 0.00657646,  0.03160213]), 0.443498950873985)\n",
      "('step 1030000:', array([ 0.0067151,  0.0314952]), 0.44571871036910937)\n",
      "('step 1040000:', array([ 0.00653812,  0.03135951]), 0.44220261169604391)\n",
      "('step 1050000:', array([ 0.00650641,  0.031375  ]), 0.44190491571774759)\n",
      "('step 1060000:', array([ 0.00668339,  0.03119506]), 0.44377648258784042)\n",
      "('step 1070000:', array([ 0.00642898,  0.03135508]), 0.44044411656281379)\n",
      "('step 1080000:', array([ 0.00667307,  0.03157853]), 0.44475781947294385)\n",
      "('step 1090000:', array([ 0.00647027,  0.03132043]), 0.44096622686257569)\n",
      "('step 1100000:', array([ 0.00647101,  0.031375  ]), 0.44157578676075726)\n",
      "('step 1110000:', array([ 0.00636482,  0.03149299]), 0.44053789844476826)\n",
      "('step 1120000:', array([ 0.00656024,  0.03140007]), 0.44295951233516867)\n",
      "('step 1130000:', array([ 0.00645331,  0.03145685]), 0.44135565266714805)\n",
      "('step 1140000:', array([ 0.00650641,  0.03136541]), 0.44196749430547616)\n",
      "('step 1150000:', array([ 0.00649977,  0.03124816]), 0.44115744297722032)\n",
      "('step 1160000:', array([ 0.00653222,  0.0312688 ]), 0.44182601263425375)\n",
      "('step 1170000:', array([ 0.00646216,  0.03132706]), 0.4410973305676526)\n",
      "('step 1180000:', array([ 0.006398  ,  0.03135656]), 0.44049719679997174)\n",
      "('step 1190000:', array([ 0.00668413,  0.0314185 ]), 0.44394978643434985)\n",
      "('step 1200000:', array([ 0.00660522,  0.03147897]), 0.44315442041417635)\n",
      "('step 1210000:', array([ 0.00654623,  0.03128355]), 0.44257313256004777)\n",
      "('step 1220000:', array([ 0.00658163,  0.03117515]), 0.44175205573158155)\n",
      "('step 1230000:', array([ 0.00642381,  0.03139491]), 0.4412548966910238)\n",
      "('step 1240000:', array([ 0.00649018,  0.03145833]), 0.44172300913018869)\n",
      "('step 1250000:', array([ 0.00651452,  0.03133222]), 0.44201067120548676)\n",
      "('step 1260000:', array([ 0.00643561,  0.0311958 ]), 0.44042995384825334)\n",
      "('step 1270000:', array([ 0.00643414,  0.03136393]), 0.44264154482405432)\n",
      "('step 1280000:', array([ 0.00654402,  0.03134476]), 0.44238979198638817)\n",
      "('step 1290000:', array([ 0.00646142,  0.03133739]), 0.44122878760859363)\n",
      "('step 1300000:', array([ 0.00653738,  0.03129904]), 0.44193691422539572)\n",
      "('step 1310000:', array([ 0.0064924 ,  0.03133296]), 0.44143635579499724)\n",
      "('step 1320000:', array([ 0.00649682,  0.03135287]), 0.44243643707538599)\n",
      "('step 1330000:', array([ 0.00651599,  0.03133001]), 0.44186210142520249)\n",
      "('step 1340000:', array([ 0.00647691,  0.03136246]), 0.44171357328610683)\n",
      "('step 1350000:', array([ 0.00652484,  0.03156083]), 0.44210268440706507)\n",
      "('step 1360000:', array([ 0.00656319,  0.03222969]), 0.44497105416088678)\n",
      "('step 1370000:', array([ 0.00658752,  0.03154387]), 0.44270378181372133)\n",
      "('step 1380000:', array([ 0.00664799,  0.03126364]), 0.4432224933389336)\n",
      "('step 1390000:', array([ 0.00657499,  0.031375  ]), 0.44283282946566427)\n",
      "('step 1400000:', array([ 0.00663693,  0.0315859 ]), 0.4446512245258466)\n",
      "('step 1410000:', array([ 0.00662661,  0.03152691]), 0.44413620454655667)\n",
      "('step 1420000:', array([ 0.00639137,  0.03119285]), 0.43995702474571269)\n",
      "('step 1430000:', array([ 0.00640317,  0.0318064 ]), 0.44195334752288989)\n",
      "('step 1440000:', array([ 0.00655139,  0.03155493]), 0.44360505429212149)\n",
      "('step 1450000:', array([ 0.00651231,  0.03147824]), 0.442402505294968)\n",
      "('step 1460000:', array([ 0.00652042,  0.03145095]), 0.44162851913624768)\n",
      "('step 1470000:', array([ 0.0064924,  0.0313691]), 0.44117028106003953)\n",
      "('step 1480000:', array([ 0.00666348,  0.03133812]), 0.44467722865864162)\n",
      "('step 1490000:', array([ 0.00651009,  0.0312865 ]), 0.4414290927924846)\n",
      "('step 1500000:', array([ 0.00644815,  0.03149667]), 0.4415910906452184)\n",
      "('step 1510000:', array([ 0.00665389,  0.0313809 ]), 0.44383269623412064)\n",
      "('step 1520000:', array([ 0.00658089,  0.0313101 ]), 0.44366302306028432)\n",
      "('step 1530000:', array([ 0.00661481,  0.03148709]), 0.44314482247299919)\n",
      "('step 1540000:', array([ 0.00647396,  0.03126954]), 0.44115908145974075)\n",
      "('step 1550000:', array([ 0.00650051,  0.03131748]), 0.44150281914413164)\n",
      "('step 1560000:', array([ 0.00661923,  0.0313101 ]), 0.44376884147136875)\n",
      "('step 1570000:', array([ 0.006589  ,  0.03144579]), 0.44265507169682072)\n",
      "('step 1580000:', array([ 0.0063921 ,  0.03123341]), 0.44015229328255034)\n",
      "('step 1590000:', array([ 0.00659564,  0.03126143]), 0.44283254415142931)\n",
      "('step 1600000:', array([ 0.00682203,  0.03132337]), 0.4471836778675542)\n",
      "('step 1610000:', array([ 0.00641865,  0.0313514 ]), 0.44051875916985134)\n",
      "('step 1620000:', array([ 0.00650862,  0.03132706]), 0.44127004068399894)\n",
      "('step 1630000:', array([ 0.00650124,  0.03124447]), 0.44151320020698887)\n",
      "('step 1640000:', array([ 0.00650567,  0.03136098]), 0.44157892735105003)\n",
      "('step 1650000:', array([ 0.00649977,  0.0312312 ]), 0.44129550118436622)\n",
      "('step 1660000:', array([ 0.00642013,  0.03127028]), 0.44057060631260947)\n",
      "('step 1670000:', array([ 0.00647838,  0.03129904]), 0.44131482149397133)\n",
      "('step 1680000:', array([ 0.00649166,  0.03209768]), 0.44457498093076026)\n",
      "('step 1690000:', array([ 0.00666422,  0.03142662]), 0.44397321335827222)\n",
      "('step 1700000:', array([ 0.00650124,  0.03124226]), 0.44120063626611594)\n",
      "('step 1710000:', array([ 0.00646216,  0.03135877]), 0.44078680077163118)\n",
      "('step 1720000:', array([ 0.00649977,  0.03123931]), 0.44136484359728118)\n",
      "('step 1730000:', array([ 0.00654918,  0.03146275]), 0.44229525520582846)\n",
      "('step 1740000:', array([ 0.00639137,  0.03126291]), 0.44032279559611637)\n",
      "('step 1750000:', array([ 0.00653148,  0.03132559]), 0.44182486806396865)\n",
      "('step 1760000:', array([ 0.00664726,  0.03129093]), 0.44329855143942415)\n",
      "('step 1770000:', array([ 0.00644667,  0.03139564]), 0.44123682366608663)\n",
      "('step 1780000:', array([ 0.00650936,  0.03137057]), 0.44125180518680163)\n",
      "('step 1790000:', array([ 0.00653295,  0.03151068]), 0.44246202749017538)\n",
      "('step 1800000:', array([ 0.00675861,  0.03132411]), 0.44533072029910975)\n",
      "('step 1810000:', array([ 0.00653148,  0.03124668]), 0.44178785595767311)\n",
      "('step 1820000:', array([ 0.00646069,  0.03127913]), 0.4404639883352352)\n",
      "('step 1830000:', array([ 0.00658384,  0.03161245]), 0.44368189307577949)\n",
      "('step 1840000:', array([ 0.00664873,  0.03140007]), 0.44434552316684478)\n",
      "('step 1850000:', array([ 0.00671658,  0.03136983]), 0.44496987280554273)\n",
      "('step 1860000:', array([ 0.00652411,  0.03137057]), 0.44174537835199595)\n",
      "('step 1870000:', array([ 0.00665979,  0.0312194 ]), 0.44378287442062764)\n",
      "('step 1880000:', array([ 0.00645331,  0.0312806 ]), 0.44029301044653074)\n",
      "('step 1890000:', array([ 0.00664136,  0.03150773]), 0.44436245878456193)\n",
      "('step 1900000:', array([ 0.00641791,  0.03132116]), 0.44049292590160893)\n",
      "('step 1910000:', array([ 0.00652042,  0.03133591]), 0.44194222835713309)\n",
      "('step 1920000:', array([ 0.00641791,  0.03153281]), 0.44086260659843024)\n",
      "('step 1930000:', array([ 0.00661334,  0.03138827]), 0.44353485554813027)\n",
      "('step 1940000:', array([ 0.00643193,  0.03143842]), 0.4410708244497219)\n",
      "('step 1950000:', array([ 0.00650567,  0.03143473]), 0.44176939205100585)\n",
      "('step 1960000:', array([ 0.0066008 ,  0.03129904]), 0.44252522900916563)\n",
      "('step 1970000:', array([ 0.00659932,  0.03121202]), 0.44260588752794355)\n",
      "('step 1980000:', array([ 0.00644077,  0.03136762]), 0.44098479778103311)\n",
      "('step 1990000:', array([ 0.00640612,  0.03127397]), 0.44024545755222988)\n",
      "('step 2000000:', array([ 0.00669372,  0.03150036]), 0.44446761133451551)\n",
      "('step 2010000:', array([ 0.00649608,  0.0314539 ]), 0.44129995629693608)\n",
      "('step 2020000:', array([ 0.00652263,  0.03130936]), 0.44219441574185608)\n",
      "('step 2030000:', array([ 0.00657794,  0.03118695]), 0.44180686986619594)\n",
      "('step 2040000:', array([ 0.00641275,  0.03135066]), 0.44060138348852163)\n",
      "('step 2050000:', array([ 0.00646511,  0.03145316]), 0.44147733660377125)\n",
      "('step 2060000:', array([ 0.00656614,  0.03132264]), 0.44214451140344763)\n",
      "('step 2070000:', array([ 0.00641644,  0.03121645]), 0.44042528843444961)\n",
      "('step 2080000:', array([ 0.00640759,  0.03131084]), 0.44150205077447413)\n",
      "('step 2090000:', array([ 0.00653664,  0.03136246]), 0.44186647780277499)\n",
      "('step 2100000:', array([ 0.00651526,  0.0313514 ]), 0.44205203464459464)\n",
      "('step 2110000:', array([ 0.00657868,  0.03130641]), 0.44276521945986502)\n",
      "('step 2120000:', array([ 0.00646806,  0.03126069]), 0.44072387040930089)\n",
      "('step 2130000:', array([ 0.00646954,  0.0313632 ]), 0.44220083933070159)\n",
      "('step 2140000:', array([ 0.00654107,  0.03129093]), 0.44241462112104485)\n",
      "('step 2150000:', array([ 0.00642013,  0.0314775 ]), 0.44140568222766574)\n",
      "('step 2160000:', array([ 0.00658384,  0.03169357]), 0.44337203014787974)\n",
      "('step 2170000:', array([ 0.0066185 ,  0.03222526]), 0.44564456813129383)\n",
      "('step 2180000:', array([ 0.00652042,  0.03141777]), 0.44184450067245296)\n",
      "('step 2190000:', array([ 0.00670404,  0.03130051]), 0.44403104870570714)\n",
      "('step 2200000:', array([ 0.00654992,  0.03144579]), 0.443059222243673)\n",
      "('step 2210000:', array([ 0.00657573,  0.03164047]), 0.4442568881981675)\n",
      "('step 2220000:', array([ 0.0066067 ,  0.03146791]), 0.44345296552022895)\n",
      "('step 2230000:', array([ 0.0064216 ,  0.03127839]), 0.44035076712592558)\n",
      "('step 2240000:', array([ 0.00642824,  0.03181893]), 0.442388436386218)\n",
      "('step 2250000:', array([ 0.00656024,  0.03161687]), 0.4435452430672589)\n",
      "('step 2260000:', array([ 0.00648355,  0.03146349]), 0.44205412255340343)\n",
      "('step 2270000:', array([ 0.00655139,  0.03145906]), 0.44210032952159556)\n",
      "('step 2280000:', array([ 0.00648871,  0.03139564]), 0.44164419684339018)\n",
      "('step 2290000:', array([ 0.00667602,  0.03132337]), 0.44488643805890965)\n",
      "('step 2300000:', array([ 0.00650124,  0.03124668]), 0.44110804568926093)\n",
      "('step 2310000:', array([ 0.00648723,  0.0313809 ]), 0.44177477692651723)\n",
      "('step 2320000:', array([ 0.00653  ,  0.0313042]), 0.44168316876811559)\n",
      "('step 2330000:', array([ 0.00649682,  0.03146201]), 0.4425452690305382)\n",
      "('step 2340000:', array([ 0.00664062,  0.03155124]), 0.44381561200764469)\n",
      "('step 2350000:', array([ 0.0064865 ,  0.03127692]), 0.44148752434944522)\n",
      "('step 2360000:', array([ 0.0065359 ,  0.03143547]), 0.44200369730479983)\n",
      "('step 2370000:', array([ 0.00666348,  0.03134771]), 0.44480103618731881)\n",
      "('step 2380000:', array([ 0.00664799,  0.03139564]), 0.44323923135045817)\n",
      "('step 2390000:', array([ 0.00640833,  0.03119285]), 0.44012894792110091)\n",
      "('step 2400000:', array([ 0.00658605,  0.03126143]), 0.44270130509750616)\n",
      "('step 2410000:', array([ 0.00673206,  0.03134329]), 0.44588850778817996)\n",
      "('step 2420000:', array([ 0.00646437,  0.0313042 ]), 0.44111833922009552)\n",
      "('step 2430000:', array([ 0.00649977,  0.03133739]), 0.44081558069034427)\n",
      "('step 2440000:', array([ 0.00648281,  0.03127028]), 0.4408994726267923)\n",
      "('step 2450000:', array([ 0.00648502,  0.0313101 ]), 0.44148016943082424)\n",
      "('step 2460000:', array([ 0.00649535,  0.0312865 ]), 0.44144609042894112)\n",
      "('step 2470000:', array([ 0.00637957,  0.03142588]), 0.44051386105368406)\n",
      "('step 2480000:', array([ 0.00656466,  0.03133001]), 0.44242204097338439)\n",
      "('step 2490000:', array([ 0.00658015,  0.03197453]), 0.44598920139645898)\n",
      "('step 2500000:', array([ 0.00662735,  0.03142367]), 0.44289850140805004)\n",
      "('step 2510000:', array([ 0.00647543,  0.0312135 ]), 0.44091645823989833)\n",
      "('step 2520000:', array([ 0.0064511 ,  0.03128134]), 0.44047262855777713)\n",
      "('step 2530000:', array([ 0.00653369,  0.03123341]), 0.44177875494525209)\n",
      "('step 2540000:', array([ 0.00653369,  0.03150921]), 0.44209607616686936)\n",
      "('step 2550000:', array([ 0.00640612,  0.03124668]), 0.44038351743331244)\n",
      "('step 2560000:', array([ 0.00660006,  0.03142735]), 0.44301424710824933)\n",
      "('step 2570000:', array([ 0.00674239,  0.03132337]), 0.44526538846571262)\n",
      "('step 2580000:', array([ 0.00644815,  0.03147529]), 0.44132238796355228)\n",
      "('step 2590000:', array([ 0.00652484,  0.03135287]), 0.44136805448089977)\n",
      "('step 2600000:', array([ 0.00650862,  0.03140154]), 0.44211689572473251)\n",
      "('step 2610000:', array([ 0.00672911,  0.03134771]), 0.44454686386340253)\n",
      "('step 2620000:', array([ 0.00649461,  0.0312806 ]), 0.44126408327336886)\n",
      "('step 2630000:', array([ 0.00643045,  0.03122161]), 0.44034315063731139)\n",
      "('step 2640000:', array([ 0.00659785,  0.03159991]), 0.44332840465498619)\n",
      "('step 2650000:', array([ 0.00657204,  0.03139491]), 0.44291589504676937)\n",
      "('step 2660000:', array([ 0.00686923,  0.03139196]), 0.44774554962465807)\n",
      "('step 2670000:', array([ 0.00652853,  0.03132337]), 0.44166488722834013)\n",
      "('step 2680000:', array([ 0.00665168,  0.03126659]), 0.44374145645712454)\n",
      "('step 2690000:', array([ 0.00645184,  0.03126217]), 0.4404747706126747)\n",
      "('step 2700000:', array([ 0.00663915,  0.03156599]), 0.44374095019604287)\n",
      "('step 2710000:', array([ 0.00640022,  0.03133075]), 0.44034951104686565)\n",
      "('step 2720000:', array([ 0.00652632,  0.03125848]), 0.44153160814522835)\n",
      "('step 2730000:', array([ 0.00644889,  0.03152396]), 0.44116903029164778)\n",
      "('step 2740000:', array([ 0.00660227,  0.03138532]), 0.44354845253928132)\n",
      "('step 2750000:', array([ 0.00644446,  0.0314185 ]), 0.44102386307127067)\n",
      "('step 2760000:', array([ 0.00651083,  0.03168398]), 0.44247236988083583)\n",
      "('step 2770000:', array([ 0.00667749,  0.03128872]), 0.44362001979281085)\n",
      "('step 2780000:', array([ 0.00659564,  0.03120096]), 0.44247159169543865)\n",
      "('step 2790000:', array([ 0.00641128,  0.03136983]), 0.44100080483703785)\n",
      "('step 2800000:', array([ 0.00650124,  0.0313337 ]), 0.44142199921364161)\n",
      "('step 2810000:', array([ 0.00672469,  0.03153649]), 0.44528130164585444)\n",
      "('step 2820000:', array([ 0.00651526,  0.03142809]), 0.44115698436187417)\n",
      "('step 2830000:', array([ 0.00649756,  0.03136025]), 0.44228353692692934)\n",
      "('step 2840000:', array([ 0.00660817,  0.03126069]), 0.44222802114449639)\n",
      "('step 2850000:', array([ 0.00641423,  0.03132559]), 0.44045830268904179)\n",
      "('step 2860000:', array([ 0.00654918,  0.03149004]), 0.44259568082801681)\n",
      "('step 2870000:', array([ 0.00654918,  0.03127249]), 0.44205099508805334)\n",
      "('step 2880000:', array([ 0.00642676,  0.03125406]), 0.44056244253952831)\n",
      "('step 2890000:', array([ 0.00639432,  0.03130051]), 0.4405150296644183)\n",
      "('step 2900000:', array([ 0.00652116,  0.03135361]), 0.44210169698791718)\n",
      "('step 2910000:', array([ 0.00649461,  0.03137647]), 0.44180783383304789)\n",
      "('step 2920000:', array([ 0.00658826,  0.03120465]), 0.44296418090183076)\n",
      "('step 2930000:', array([ 0.00646511,  0.03123046]), 0.44064636998897311)\n",
      "('step 2940000:', array([ 0.00645921,  0.0313455 ]), 0.4417168948492034)\n",
      "('step 2950000:', array([ 0.00653443,  0.03129167]), 0.44224358862482882)\n",
      "('step 2960000:', array([ 0.00643414,  0.03162867]), 0.44185017257899989)\n",
      "('step 2970000:', array([ 0.00662808,  0.03157632]), 0.44406110809235239)\n",
      "('step 2980000:', array([ 0.00665021,  0.03197453]), 0.44529882548308497)\n",
      "('step 2990000:', array([ 0.00650198,  0.03127544]), 0.44149390850896075)\n",
      "('step 3000000:', array([ 0.00666348,  0.03128355]), 0.44318926612033482)\n",
      "('step 3010000:', array([ 0.00649756,  0.03147307]), 0.4424027148821702)\n",
      "('step 3020000:', array([ 0.00654992,  0.03160581]), 0.44345835198909594)\n",
      "('step 3030000:', array([ 0.00658015,  0.03158811]), 0.44328256119584381)\n",
      "('step 3040000:', array([ 0.00646364,  0.03129019]), 0.4408937634398894)\n",
      "('step 3050000:', array([ 0.00646511,  0.03173781]), 0.44330921906253457)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-8d2b54ced28a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_output\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_output\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         _, l, predictions = session.run(\n\u001b[0;32m---> 19\u001b[0;31m             [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mW0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 1000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_output.shape[0] - batch_size)\n",
    "        #offset = 0\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_input[offset:(offset + batch_size), :]\n",
    "        batch_output = train_output[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_output : batch_output}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 10000 == 0):\n",
    "            W0 = weights.eval()\n",
    "            B0 = biases.eval()\n",
    "            W1 = weights2.eval()\n",
    "            B1 = biases2.eval()\n",
    "            #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            pred = train_prediction.eval(feed_dict = {tf_train_dataset: train_input, tf_train_output: train_output})\n",
    "            tpred = test_prediction.eval()\n",
    "            #print(np.mean(abs(pred - train_output)>0.5000, axis=0))\n",
    "            #print(my_mse(pred, train_output))\n",
    "            print(\"step %d:\" % step, np.mean(abs(pred - train_output)>0.5000, axis=0), my_mse(pred, train_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.28951739e-02,   1.65119004e-02,   1.01238862e-02,\n",
       "          8.55314210e-02,   6.22863695e-02,   5.94221503e-02,\n",
       "          2.54944582e-02,  -8.97842348e-02,   2.48227324e-02,\n",
       "         -4.25750315e-02,   1.41070848e-02,   1.01724721e-01,\n",
       "          1.80129543e-01,   2.57834122e-02,  -3.53035986e-01,\n",
       "          7.45099261e-02,  -2.10935637e-01,  -3.21772397e-01,\n",
       "         -8.96112502e-01,  -2.04263225e-01,   1.73088157e+00,\n",
       "         -8.18579197e-01,   1.75526047e+00,   1.49274802e+00,\n",
       "          1.63044304e-01,  -8.01915348e-01,   4.12034504e-02,\n",
       "          1.17459960e-01,   9.67575517e-03,   3.42897512e-02,\n",
       "          1.91381034e-02,  -1.21345274e-01,   6.57919347e-02,\n",
       "         -1.53688330e-03,  -4.90540527e-02,   2.74076443e-02,\n",
       "         -9.34747681e-02,  -9.96665880e-02,  -4.31297302e-01,\n",
       "         -7.20995246e-03,  -9.64779079e-01,  -2.02184841e-02,\n",
       "          2.81997681e-01,   1.10685110e-01,  -3.71672779e-01,\n",
       "          1.36981398e-01,  -3.80686831e+00,  -6.70365095e-01,\n",
       "         -8.07451606e-01,   2.45330468e-01,   8.02116096e-01,\n",
       "         -2.58818007e+00,  -1.62606299e-01,   5.93526840e-01,\n",
       "          8.55537295e-01,  -4.00257289e-01,  -2.85130385e-02,\n",
       "         -6.21825993e-01,  -5.61705828e-01,  -2.80691504e-01,\n",
       "         -1.24386765e-01,  -8.79820645e-01,  -5.21839917e-01,\n",
       "          2.30891965e-02,  -1.65400058e-01,   2.06495285e-01,\n",
       "         -3.92396986e-01,   4.54536900e-02,  -1.19954990e-02,\n",
       "          6.72550321e-01,  -7.27340952e-02,  -2.98260152e-01,\n",
       "         -1.67611992e+00,  -5.51225138e+00,   1.20696139e+00,\n",
       "          1.05799520e+00,   3.82419638e-02,   8.60897124e-01,\n",
       "          1.34107545e-02,   1.41055167e-01,   1.53182030e-01,\n",
       "          8.35369751e-02,  -4.31375392e-02,  -1.55012846e-01,\n",
       "         -1.23814024e-01,  -7.08684921e-02,  -2.03394219e-02,\n",
       "         -6.19063340e-02,  -9.46520120e-02,   6.49479358e-03,\n",
       "         -9.93606001e-02,   3.58918071e-01,   6.93840161e-02,\n",
       "          1.39714077e-01,   4.88971025e-02,  -4.49669920e-02,\n",
       "          2.14809641e-01,  -7.80043975e-02,   3.03254485e+00,\n",
       "          2.60110402e+00,  -3.49856764e-01,  -4.66974878e+00,\n",
       "         -8.06848407e-01,   6.91021860e-01,   6.60442095e-03,\n",
       "         -1.82391152e-01,  -6.63308725e-02,  -1.43962488e-01,\n",
       "         -7.26790875e-02,   2.36854598e-01,  -2.92062527e-03,\n",
       "          1.43008426e-01,   6.77474588e-03,   1.14755243e-01,\n",
       "          2.25039236e-02,  -4.13146913e-02,  -6.67329729e-01,\n",
       "         -2.15591222e-01,  -5.67228377e-01,   1.47949293e-01,\n",
       "          2.10254058e-01,   2.69279629e-01,   6.94706142e-01,\n",
       "         -1.63381189e-01,  -3.44575256e-01,   1.40568423e+00,\n",
       "         -7.13483155e-01,  -1.20227671e+00,  -1.56799030e+00,\n",
       "         -1.49883002e-01,  -1.73879817e-01,   2.08557278e-01,\n",
       "          1.16344402e-02,  -2.35654473e-01,  -1.27643800e+00,\n",
       "          5.69215178e-01,  -1.35144150e+00,   1.05920172e+00,\n",
       "         -1.02303612e+00,   3.89608204e-01,  -1.28569150e+00,\n",
       "          5.31385303e-01,   1.07519221e+00,   3.96995515e-01,\n",
       "         -7.41585314e-01,  -8.75997424e-01,  -1.91559052e+00,\n",
       "         -1.39489025e-01,   1.46542263e+00,   9.05828327e-02,\n",
       "          1.43116280e-01,   2.82518673e+00,  -2.40599847e+00,\n",
       "          1.44331694e+00,  -9.49428976e-01,   1.20482564e+00,\n",
       "         -9.07283202e-02,  -7.31344745e-02,   5.48897423e-02,\n",
       "         -1.14985351e-02,   3.29682797e-01,   7.69462049e-01,\n",
       "         -2.15318874e-02,   1.80490181e-01,   5.46239853e-01,\n",
       "          5.22981405e-01,  -1.09958835e-01,  -1.61937416e-01,\n",
       "          5.36470711e-01,  -7.04843551e-02,   6.42299280e-02,\n",
       "          3.53086531e-01,  -4.65101510e-01,  -2.96124309e-01,\n",
       "          9.19973757e-03,  -4.21933979e-01,  -6.48569465e-01,\n",
       "          1.14846802e+00,   1.82366979e+00,   1.58494592e+00,\n",
       "         -1.83930898e+00,  -1.85077918e+00,   3.27137336e-02,\n",
       "         -5.28560728e-02,   1.93813331e-02,   3.54694836e-02,\n",
       "         -5.41935228e-02,  -1.23448931e-01,  -1.22634739e-01,\n",
       "         -1.03123061e-01,   1.79540962e-02,   5.34792896e-04,\n",
       "         -7.54224509e-02,   1.57456286e-03,   8.62745494e-02,\n",
       "         -2.17741907e-01,   1.17468305e-01,   1.34304650e-02,\n",
       "          1.20098077e-01,   2.68362284e-01,   1.84410829e-02,\n",
       "         -1.14277132e-01,   6.95449054e-01,  -2.37080741e+00,\n",
       "          5.34873426e-01,   3.97916746e+00,  -7.79462039e-01,\n",
       "         -1.61229837e+00,   6.76166713e-02,  -7.67419562e-02,\n",
       "          7.13149756e-02,  -6.42002523e-02,  -3.74417333e-03,\n",
       "          5.03872298e-02,   2.70444411e-03,   5.53370677e-02,\n",
       "         -1.45802442e-02,   4.50725947e-03,  -6.91427216e-02,\n",
       "         -5.67613617e-02,  -3.99227403e-02,   4.51298691e-02,\n",
       "         -1.33792207e-01,  -1.91231310e-01,  -1.24700442e-02,\n",
       "         -2.07976043e-01,   5.49214542e-01,   1.25105634e-01,\n",
       "          3.78124619e+00,  -1.59552777e+00,  -1.05001867e+00,\n",
       "          4.92427111e+00,   1.62778866e+00,   7.84031749e-01,\n",
       "         -7.23429620e-02,   4.38330881e-02,   1.55165754e-02,\n",
       "         -6.57584518e-02,  -6.04013763e-02,  -4.22390327e-02,\n",
       "         -1.03063039e-01,   2.13701092e-02,  -1.50651457e-02,\n",
       "          1.47113856e-02,  -3.85852307e-02,   3.16223875e-02,\n",
       "          4.92997393e-02,   5.96507668e-01,   2.30747804e-01,\n",
       "          1.98664963e-02,  -2.12604571e-02,  -6.17754638e-01,\n",
       "         -3.23477566e-01,   1.03024602e-01,   9.87573117e-02,\n",
       "          1.64755797e+00,  -1.08032537e+00,   1.29163873e+00,\n",
       "          3.56825185e+00,  -2.20517874e+00,   9.20519084e-02,\n",
       "         -4.59440239e-02,   1.48425758e-01,   1.81137368e-01,\n",
       "         -4.97632176e-02,   9.12531614e-02,  -5.63491769e-02,\n",
       "         -5.91781363e-02,  -9.47716907e-02,  -3.75089012e-02,\n",
       "         -1.07488394e-01,  -3.24692309e-01,  -1.02506086e-01,\n",
       "         -1.44310951e-01,   4.04604733e-01,   7.03562617e-01,\n",
       "          3.36726099e-01,   8.99136007e-01,   5.31530619e-01,\n",
       "         -8.98030400e-01,  -3.65766644e+00,  -2.89670062e+00,\n",
       "          2.14757562e+00,   1.86337009e-02,   3.90942931e-01,\n",
       "         -1.91688761e-01,  -8.94278213e-02,   8.10851008e-02,\n",
       "         -1.79585904e-01,   1.20850541e-02,  -3.70722217e-03,\n",
       "          1.44382380e-02,  -3.51025909e-02,  -7.39239389e-03,\n",
       "          5.46033978e-01,   1.87276810e-01,   3.60445887e-01,\n",
       "          3.63478005e-01,   3.13227177e-02,  -3.97248685e-01,\n",
       "         -3.75483692e-01,  -6.02627575e-01,  -2.93868005e-01,\n",
       "          8.53155628e-02,   2.14153484e-01,   2.08522379e-01,\n",
       "          5.94460547e-01,  -1.59529209e+00,  -8.52028489e-01,\n",
       "          4.08320665e+00,   2.84478331e+00,   6.09367073e-01,\n",
       "          2.02448550e-03,   3.59666497e-02,  -2.30953619e-02,\n",
       "         -3.03777512e-02,   1.43070258e-02,   4.28427849e-03,\n",
       "          2.92818341e-02,   5.15554585e-02,   3.14441626e-03,\n",
       "         -1.59972422e-02,   9.75555368e-03,   3.12220417e-02,\n",
       "         -9.79584828e-02,   2.68614595e-03,  -1.26094282e-01,\n",
       "         -6.35957792e-02,   8.31034854e-02,  -7.40362704e-02,\n",
       "          4.22757864e-01,   2.93876207e-03,   1.53959620e+00,\n",
       "         -1.23342955e+00,   5.24474159e-02,   3.28589737e-01,\n",
       "          7.82403421e+00,   2.14059830e-01,   6.97582141e-02,\n",
       "          1.28337884e+00,   3.58257532e-01,   7.67516494e-01,\n",
       "         -4.74138148e-02,  -5.22553772e-02,   8.64099637e-02,\n",
       "          9.51432213e-02,  -1.44344449e-01,  -2.00019285e-01,\n",
       "         -1.99863046e-01,   1.22664087e-01,   1.23400354e+00,\n",
       "         -2.03378484e-01,   8.24471295e-01,  -5.70497960e-02,\n",
       "         -1.21214044e+00,  -3.12431842e-01,   1.33997893e+00,\n",
       "         -1.48151755e-01,  -3.35616529e-01,   9.45750773e-01,\n",
       "         -2.55414426e-01,  -1.05878520e+00,  -9.48259950e-01,\n",
       "         -1.13616586e+00,  -5.13350666e-01,  -6.71043694e-01,\n",
       "         -1.05725861e+00,   7.71392047e-01,  -3.61250013e-01,\n",
       "          7.54691243e-01,   8.17330420e-01,  -4.19688284e-01,\n",
       "          6.45704806e-01,  -1.80870950e-01,   8.74987602e-01,\n",
       "         -2.87044078e-01,   7.97963679e-01,  -3.78039151e-01,\n",
       "         -3.97121727e-01,   1.41343141e+00,  -8.84506702e-02,\n",
       "         -1.12551498e+00,  -1.94484651e+00,   8.76977146e-01,\n",
       "          4.12161589e-01,   5.16540587e-01,  -1.95372000e-01,\n",
       "         -2.15533781e+00,   1.94567466e+00,   1.33817029e+00,\n",
       "          1.16534829e+00,  -7.06851959e-01,   3.76544476e-01,\n",
       "         -1.53155997e-01,  -5.92203081e-01,   3.12804341e-01,\n",
       "         -1.79363251e-01,   2.05206424e-01,  -2.45860562e-01,\n",
       "         -8.48149136e-02,  -2.51700819e-01,  -5.70187449e-01,\n",
       "          6.32474348e-02,   1.74592100e-02,   5.99744678e-01,\n",
       "         -1.01597357e+00,  -2.74407834e-01,  -9.30796802e-01,\n",
       "         -1.17006910e+00,  -3.11489552e-01,  -1.01915133e+00,\n",
       "         -1.45726395e+00,   4.54445362e-01,   4.54103708e+00,\n",
       "          5.94891012e-01,  -8.83533537e-01]], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WW0 = np.concatenate((W0, np.reshape(B0, (1, 16))), axis=0)\n",
    "np.reshape(np.transpose(WW0), (1, 26*16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
